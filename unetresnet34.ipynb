{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom tqdm import tqdm\nfrom PIL import Image\nimport numpy as np\nfrom glob import glob","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class UNetResNet34(nn.Module):\n    def __init__(self, num_classes):\n        super().__init__()\n        self.encoder = models.resnet34(pretrained=True)\n        self.encoder_layer1 = nn.Sequential(\n            self.encoder.conv1, self.encoder.bn1, self.encoder.relu,\n            self.encoder.maxpool, self.encoder.layer1)\n        self.encoder_layer2 = self.encoder.layer2\n        self.encoder_layer3 = self.encoder.layer3\n        self.encoder_layer4 = self.encoder.layer4\n        self.upconv4 = self.upsample_block(512, 256)\n        self.upconv3 = self.upsample_block(256, 128)\n        self.upconv2 = self.upsample_block(128, 64)\n        self.upconv1 = self.upsample_block(64, 64)\n        self.final_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n\n    def upsample_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x1 = self.encoder_layer1(x)\n        x2 = self.encoder_layer2(x1)\n        x3 = self.encoder_layer3(x2)\n        x4 = self.encoder_layer4(x3)\n        d4 = self.upconv4(x4)\n        d3 = self.upconv3(d4 + x3)\n        d2 = self.upconv2(d3 + x2)\n        d1 = self.upconv1(d2 + x1)\n        out = self.final_conv(d1)\n        return out","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CMDataset(Dataset):\n    color_encoding = [\n        ('Bus Lane', (0, 255, 255)),\n        ('Cycle Lane', (0, 128, 255)),\n        ('Diamond', (178, 102, 255)),\n        ('Junction Box', (255, 255, 51)),\n        ('Left Arrow', (255, 102, 178)),\n        ('Pedestrian Crossing', (255, 255, 0)),\n        ('Right Arrow', (255, 0, 127)),\n        ('Straight Arrow', (255, 0, 255)),\n        ('Slow', (0, 255, 0)),\n        ('Straight-Left Arrow', (255, 128, 0)),\n        ('Straight-Right Arrow', (255, 0, 0)),\n        ('Background', (0, 0, 0))\n    ]\n\n    def __init__(self, mode='train', num_classes=12):\n        self.mode = mode\n        self.num_classes = num_classes\n        self.normalize = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n        ])\n        self.data_files = self.get_files(os.path.join(os.getcwd(), f'{mode}/'))\n        self.label_files = [f.replace(mode, f\"{mode}_labels\") for f in self.data_files]\n\n    def get_files(self, folder):\n        return glob(f\"{folder}/*.jpg\")\n\n    def __len__(self):\n        return len(self.data_files)\n\n    def __getitem__(self, index):\n        data = Image.open(self.data_files[index]).resize((512, 512))\n        label = Image.open(self.label_files[index]).resize((512, 512))\n        data = self.normalize(data)\n        label = self.one_hot_encode(np.array(label))\n        return data, torch.tensor(label, dtype=torch.long)\n\n    def one_hot_encode(self, label):\n        semantic_map = np.zeros(label.shape[:2], dtype=int)\n        for class_index, (_, color) in enumerate(self.color_encoding):\n            equality = np.all(label == color, axis=-1)\n            semantic_map[equality] = class_index\n        return semantic_map","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, dataloader, optimizer, loss_func):\n    model.train()\n    tq = tqdm(total=len(dataloader) * dataloader.batch_size)\n    tq.set_description('Training')\n    for data, label in dataloader:\n        data, label = data.cuda(), label.cuda()\n        output = model(data)\n        loss = loss_func(output, label)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tq.set_postfix(loss=f'{loss.item():.6f}')\n        tq.update(dataloader.batch_size)\n    tq.close()\n\ndef validate(model, dataloader):\n    model.eval()\n    total_accuracy = 0\n    with torch.no_grad():\n        for data, label in dataloader:\n            data, label = data.cuda(), label.cuda()\n            output = model(data)\n            output = torch.argmax(output, dim=1)\n            accuracy = (output == label).float().mean().item()\n            total_accuracy += accuracy\n    return total_accuracy / len(dataloader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_classes = 12\nmodel = UNetResNet34(num_classes=num_classes).cuda()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\nloss_func = nn.CrossEntropyLoss()\n\ntrain_dataset = CamVidDataset(mode='train')\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, drop_last=True)\n\nval_dataset = CamVidDataset(mode='val')\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 100\nCHECKPOINT_STEP = 1\nVALIDATE_STEP = 1\nmax_miou = 0\n\nfor epoch in range(EPOCHS):\n    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n    train(model, train_loader, optimizer, loss_func)\n    if epoch % VALIDATE_STEP == 0:\n        val_accuracy = validate(model, val_loader)\n        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n    if epoch % CHECKPOINT_STEP == 0:\n        torch.save(model.state_dict(), f'checkpoint_epoch_{epoch}.pth')","metadata":{},"execution_count":null,"outputs":[]}]}